import { Meta } from '@storybook/addon-docs/blocks';

<Meta title="Testing/Framework/Coverage Matrix/Overview" tags={['pending-review']} />

# Matrix Generation for Component Permutations
<div style={{display: 'inline-block', padding: '4px 12px', background: '#ffe066', color: '#7c5c00', borderRadius: '16px', fontWeight: 'bold', marginBottom: '16px'}}>⏳ Pending Review</div>

## Why Matrix-Driven Static Analysis Coverage Is Essential

Traditional unit test coverage tools (like Istanbul/NYC) are effective for verifying the compiler's internal logic, but they cannot ensure that every possible decorator permutation is exercised in real-world usage. This is because, to truly flex the compiler, we must use it as an external consumer—generating components and running them through the build/test pipeline. Standard coverage tools cannot track which decorator permutations are exercised in generated code.

Matrix-driven static analysis bridges this gap:
- **Permutation Matrix:** For each decorator, all possible permutations (based on its properties/options) are enumerated in a machine-readable matrix (typically an `.mdx` file).
- **Static Analysis:** Scripts scan the codebase to check which permutations are present in generated components and tests.
- **Coverage Reporting:** The matrix is updated to show which permutations are covered and which are missing.
- **Test Generation:** Missing permutations trigger new component/test generation, ensuring every case is exercised.
- **Expansion:** While currently focused on Component Tests, this approach will expand to e2e tests, ensuring full coverage across all test types.

This matrix-driven strategy is essential for maintaining confidence in Stencil’s feature coverage, especially as new decorator options are added. It complements runtime tests and standard coverage tools, providing a systematic way to achieve and maintain comprehensive coverage.

## Core Objective

The primary goal of each `*-coverage.js` script is to determine the component permutation coverage for a specific Stencil decorator (e.g., `@Prop`, `@Listen`). It achieves this by performing two key functions:

1.  **Defining the entire universe of possible feature permutations.**
2.  **Scanning the codebase to see which of those permutations are actually implemented in dedicated test components.**

The final output is a comparison between the "possible" and the "actual," resulting in a coverage percentage and a list of what's missing.

## The Generation Process: A Step-by-Step Guide

All coverage scripts follow a similar underlying logic, which can be broken down into the following steps:

### 1. Defining the Scope

-   **Component Directories:** Each script begins by defining a list of directories to scan for component files. This is typically `test/wdio` and `test/end-to-end`.
-   **Permutation Options:** The script hardcodes the "source of truth" for a decorator's permutations. This includes all possible properties and their potential values.
    -   **Example (`@Listen`):**
        -   `TARGET_OPTIONS`: `['window', 'document', 'body', 'parent', 'host']`
        -   `capture`: `[true, false]`

### 2. Finding Relevant Component Files

A file system utility recursively searches the specified component directories for files with a specific extension, almost always `.tsx`. This produces a list of all component files that could contain decorator implementations.

### 3. Parsing Source Code to Find Decorators

This is the most critical and variable step. The script reads the content of each component file and uses a specific strategy to find all instances of the target decorator.

-   **Strategy A: Regex for Simple Decorators:** For decorators with a straightforward, single-line syntax like `@Listen`, a single, effective regex (`/@Listen\s*\(([^)]*)\)/g`) is used to capture all instances and their options at once. This is efficient and concise.

-   **Strategy B: Line-by-Line Parsing for Complex Decorators:** For decorators like `@Prop` where the declaration can be multi-line and associated with a typed class property, a more robust line-by-line parsing approach is necessary. The script iterates through the file's lines, identifies a decorator, and then continues reading subsequent lines to piece together the full context (e.g., the property's type, name, and initializer).

### 4. Extracting and Normalizing Data

Once a decorator is found, its options are extracted.

-   **Extraction:** Regex is used to pull out the values of specific keys (e.g., `reflect: true`, `target: 'window'`).
-   **Defaults:** The script logic accounts for default values. For instance, in `@Listen`, if `target` is not specified, it defaults to `'host'`.
-   **Normalization:** The extracted values are normalized into a consistent format for easy comparison. A common example is converting boolean `true`/`false` into checkmark icons (`✓`/`✗`).

### 5. Calculating Coverage

-   The script programmatically generates a complete list of all possible permutation keys from the hardcoded options (e.g., `"window|✓"`, `"window|✗"`).
-   It then creates a map of all the permutations it found during the parsing step.
-   By comparing the "found" map against the "all possible" list, it calculates the total number of covered permutations, the overall percentage, and a list of the specific permutations that are missing.

### 6. Generating Output

The final results are typically written to a `.json` file (e.g., `prop-coverage-data.json`). This file serves as a data source for other tools and, in many cases, is used to dynamically build and update the coverage matrix tables displayed in the `.mdx` documentation files.

## Real-World Example: The `@Prop` Decorator

Our first application of this strategy to the `@Prop` decorator provides a perfect case study of the iterative process in action and highlights the exponential cost of comprehensive testing. For a concrete example of the permutation matrix in practice, see [@Prop Matrix: Covered Permutations](?path=/docs/testing-decorators-prop--docs#covered-permutations).

1.  **Initial Coverage Push**: We began by identifying the primary permutations for `@Prop`, focusing on its `type` (e.g., `string`, `number`, `boolean`) and the `reflect` option. The AI assistant generated a suite of isolated component tests in `test/wdio/prop-reflect/` to cover these cases. After this initial push, our coverage script reported **100% coverage** for the known permutations.
2.  **Discovering a Blind Spot**: Upon review, we realized we had completely overlooked the `mutable` property. It was not being tracked by our coverage script, giving us a false sense of security.
3.  **The Cost of a New Permutation**: We updated the `prop-coverage.js` script to recognize the `mutable` property. The impact was immediate and dramatic. The total number of testable permutations doubled, and our coverage plummeted from **100% to 62.5%**. This demonstrated a critical lesson: each new property added to a feature doesn't just add a few more tests; it can exponentially increase the testing matrix.
4.  **The Next Iteration**: We are now back at the beginning of the testing loop, but we are not starting from scratch. With a more accurate permutation matrix, we will now use the same strategy—and the lessons learned from the first iteration—to generate the tests required to cover the new `mutable` permutations and bring our coverage back to 100%.

This experience validates our strategy. The loop is designed to be self-correcting. It allows us to uncover our own blind spots, systematically address them, and continuously improve the robustness of our test suite.

# Coverage Matrix: Shared Context for AI and Humans

This documentation is the source of truth for the coverage matrix workflow. It is designed to be concise and clear for both AI and human readers, enabling iterative evaluation, generation, and verification of coverage.

- **Generation:** See `generation.mdx` for how to enumerate permutations and generate missing components/tests.
- **Verification:** See `verification.mdx` for how to confirm matrix accuracy and automate reliability checks.

The process is inherently iterative, adapting as new decorator options and features are added. Testing details (component and e2e) are covered in separate documentation files.

## Key Learnings and Implications

-   **The Script is the Source of Truth for Components:** The reliability of our entire testing loop hinges on the accuracy of these coverage scripts. If a permutation option is not defined in the script, it will not be tracked, creating a blind spot (as seen with the `@Prop({ mutable: true })` case).
-   **Parsing is Fragile:** Source code parsing, whether with regex or line-by-line analysis, can be brittle. Changes in code formatting or syntax could potentially break the scripts. They must be maintained alongside any changes to coding conventions.
-   **Human Oversight is Essential:** The process is automated but not infallible. It's crucial for developers to periodically review these scripts to ensure they accurately reflect the full feature set of the decorators they are intended to cover. When a new option is added to a decorator, the corresponding coverage script **must** be updated.
-   **Continuous Improvement:** After each major testing iteration, document learnings, mistakes, coverage gaps, and process optimizations in the appropriate `.ai` folder files and documentation. This ensures the methodology evolves and improves over time.

## Further Reading

For a high-level overview of how matrix-driven coverage fits into the overall testing strategy, see [Testing Overview](?path=/docs/testing-framework-overview--docs).
