import { Meta } from '@storybook/addon-docs/blocks';

<Meta title="Testing/Framework/Overview" tags={['pending-review']} />

# Testing Framework Overview
<div style={{display: 'inline-block', padding: '4px 12px', background: '#ffe066', color: '#7c5c00', borderRadius: '16px', fontWeight: 'bold', marginBottom: '16px'}}>‚è≥ Pending Review</div>

# Stencil.js Testing Strategy

This document outlines the testing strategy for the Stencil.js codebase. It is centered around a GenAI-powered loop to ensure a comprehensive and robust testing suite that is both maintainable and scalable.

## Philosophy

For a foundational library like Stencil, the testing strategy must be rigorous. It should not only validate that features work as expected but also ensure they are reliable in isolation and play well with others. We follow a model inspired by the classic testing pyramid.

A key aspect of our strategy is recognizing the difference between **static coverage analysis** and **runtime behavior testing**. Our coverage scripts work by statically analyzing the source code for specific patterns (e.g., the presence of a `@Prop` decorator with certain options). This is a fast and effective way to inventory our features, but it does not execute the code to verify its behavior. Therefore, our testing strategy is designed to satisfy both our static analysis tools and the need for robust, behavior-driven validation.

> **Note:** For a deeper dive into how the permutation matrix is generated and used for coverage, see [Matrix Generation](?path=/docs/testing-framework-coverage-matrix-generation--docs).

## Summary: The Testing Pyramid in Practice

<table>
  <thead>
    <tr>
      <th>Test Type</th>
      <th>Directory</th>
      <th>Technology</th>
      <th>Goal</th>
      <th>Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Core Tests</strong></td>
      <td><code>src/</code></td>
      <td>Jest</td>
      <td>Verify Stencil's internal logic and compiler functionality.</td>
      <td>[Core Testing Docs](?path=/docs/testing-framework-tests-core-tests--docs)</td>
    </tr>
    <tr>
      <td><strong>Component Tests</strong></td>
      <td><code>test/wdio/</code></td>
      <td>WebdriverIO (<code>@wdio/browser-runner/stencil</code>)</td>
      <td>Verify individual features in isolation and satisfy static analysis.</td>
      <td>[Component Testing Docs](?path=/docs/testing-framework-tests-component-tests--docs)</td>
    </tr>
    <tr>
      <td><strong>E2E Tests</strong></td>
      <td><code>test/end-to-end/</code></td>
      <td>Stencil's <code>newE2EPage()</code> (Puppeteer)</td>
      <td>Verify that features work correctly together (e.g. multiple decorators, render function) and that component behavior is correct in a browser environment.</td>
      <td>[E2E Testing Docs](?path=/docs/testing-framework-tests-e2e-tests--docs)</td>
    </tr>
  </tbody>
</table>

## The GenAI-Powered Testing Loop

A groundbreaking aspect of our testing strategy is its reliance on a GenAI-powered iterative loop. This approach allows us to systematically work towards 100% coverage of all feature permutations in a way that is both dynamic and self-improving. The loop is as follows:

1.  **Define Component Permutations**: For a given feature, like a decorator, we first study all its possible variations and options. These are documented in a human- and machine-readable permutation matrix within an `.mdx` file. This matrix serves as the single source of truth for which component permutations need to be tested. For details on how the matrix is generated, see [Matrix Generation](?path=/docs/testing-framework-coverage-matrix-generation--docs).
2.  **AI-Driven Component Test Generation**: A generative AI assistant uses the permutation matrix as a blueprint to generate the necessary component-level tests in the `test/wdio` directory. For each permutation, it creates an isolated component and a corresponding test to exercise its behavior.
3.  **Test Execution and Validation**: Before checking coverage, the newly generated `wdio` tests are run to ensure they pass. This step is critical to confirm that the generated components and tests are functionally correct and adhere to the established patterns.
4.  **Coverage Analysis & Matrix Update**: After the tests are generated and pass, a custom coverage script analyzes the codebase. It updates the permutation matrix `.mdx` file with the latest coverage statistics, clearly marking which component permutations are implemented and tested, and which are missing.
5.  **Iterate and Refine**: The AI reviews the updated matrix. If any permutations are uncovered, it generates the missing components and tests to cover the gaps. This loop continues until all permutations in the matrix are covered, ensuring our test suite is as comprehensive as possible.
6.  **Document Learnings**: Once 100% coverage is achieved, the AI documents any learnings from the testing iteration in the appropriate `.ai` folder files. This includes mistakes made during test generation, loopholes discovered in coverage scripts, improvements to documentation, and any patterns or best practices identified. This step ensures that each iteration builds upon the knowledge gained from previous cycles, making the process continuously more efficient and accurate.

This iterative process allows us to not only achieve but also maintain high test coverage over time, adapting as new features and variations are introduced while continuously improving our methodology.

## An Open, Community-Driven Framework

While the concept of a GenAI-powered testing loop is powerful, its true potential is unlocked through community collaboration. We envision this testing strategy as a public project, open to contributions from everyone.

The framework is designed to be model-agnostic. Anyone, whether they have access to cutting-edge proprietary AI or open-source models, can participate. The process is simple:

1.  Fork the repository.
2.  Run the testing loop using your preferred AI model.
3.  If your model generates new, more efficient, or more comprehensive tests, submit a pull request.

This approach allows us to harness the collective intelligence and diverse capabilities of the entire community. By crowdsourcing the test generation process, we can build a more robust, resilient, and thoroughly-vetted testing suite than any single team or model could create alone.

## Continuous Improvement

This testing strategy is a living document. We will continue to refine our approach, adopt new tools, and improve our processes over time to ensure Stencil remains a reliable and high-quality framework for all developers.

A core tenet of this continuous improvement is the practice of documenting our learnings. After each major testing iteration, we will update the relevant documentation‚Äîincluding this strategy, the component-level and end-to-end testing guides, and the [audit log](?path=/docs/testing-framework-audit--docs)‚Äîwith any new insights, patterns, or challenges we encountered. This ensures that our institutional knowledge grows with our test suite.

**Key areas for learning documentation include:**

-   **AI Mistakes and Corrections**: Recording instances where the AI generated incorrect or suboptimal tests, along with the corrections made
-   **Coverage Script Gaps**: Documenting loopholes or blind spots discovered in coverage scripts and how they were addressed
-   **Documentation Improvements**: Noting areas where guides or instructions were unclear or incomplete
-   **Process Optimizations**: Capturing workflow improvements or shortcuts discovered during iterations
-   **Pattern Recognition**: Identifying recurring themes or best practices that emerge from the testing process

As this is our first time implementing the GenAI-powered loop, we anticipate a learning period where mistakes are possible. Our commitment is to learn from these experiences and rapidly evolve towards a rock-solid automated testing framework.

By adhering to this strategy, we build a comprehensive test suite that is both robust and maintainable, ensuring that Stencil remains a reliable tool for developers.

---

<div style={{ marginTop: '3rem', textAlign: 'center' }}>
  <span style={{
    fontSize: 36,
    fontWeight: 800,
    color: '#3b82f6',
    background: '#fff',
    borderRadius: 20,
    padding: '3rem 4rem',
    display: 'inline-block',
    boxShadow: '0 8px 32px rgba(59,130,246,0.10)',
    letterSpacing: '0.03em',
    border: '3px solid #0ea5e9',
    marginBottom: '2.5rem',
    textShadow: 'none',
    position: 'relative',
    overflow: 'hidden'
  }}>
    <span style={{ display: 'block', fontSize: 28, color: '#0ea5e9', fontWeight: 900, marginBottom: 18, zIndex: 1, position: 'relative', letterSpacing: '0.04em' }}>
      üöÄ [2025 Testing Roadmap: Dive into the future of Stencil.js testing!](?path=/docs/testing-framework-roadmap-2025--docs) üöÄ
    </span>
    <span style={{ display: 'block', fontSize: 20, color: '#6366f1', fontWeight: 500, marginTop: 18, zIndex: 1, position: 'relative', letterSpacing: '0.02em' }}>
      Next-gen coverage, GenAI-powered automation, and SSR mastery await.
    </span>
  </span>
</div>
