import { Meta } from '@storybook/addon-docs/blocks';

<Meta title="Testing/Framework/Overview" tags={['pending-review']} />

# Testing Framework Overview
<div style={{display: 'inline-block', padding: '4px 12px', background: '#ffe066', color: '#7c5c00', borderRadius: '16px', fontWeight: 'bold', marginBottom: '16px'}}>‚è≥ Pending Review</div>

# Stencil.js Testing Strategy

This document outlines the testing strategy for the Stencil.js codebase. It is centered around a GenAI-powered loop to ensure a comprehensive and robust testing suite that is both maintainable and scalable.

## Philosophy

For a foundational library like Stencil, the testing strategy must be rigorous. It should not only validate that features work as expected but also ensure they are reliable in isolation and play well with others. We follow a model inspired by the classic testing pyramid.

A key aspect of our strategy is recognizing the difference between **static coverage analysis** and **runtime behavior testing**. Our coverage scripts work by statically analyzing the source code for specific patterns (e.g., the presence of a `@Prop` decorator with certain options). This is a fast and effective way to inventory our features, but it does not execute the code to verify its behavior. Therefore, our testing strategy is designed to satisfy both our static analysis tools and the need for robust, behavior-driven validation.

> **Note:** For a deeper dive into how the permutation matrix is generated and used for coverage, see [Matrix Generation](?path=/docs/testing-framework-coverage-matrix-generation--docs).

## Summary: The Testing Pyramid in Practice

<table>
  <thead>
    <tr>
      <th>Test Type</th>
      <th>Directory</th>
      <th>Technology</th>
      <th>Goal</th>
      <th>Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Core Tests</strong></td>
      <td><code>src/</code></td>
      <td>Jest</td>
      <td>Verify Stencil's internal logic and compiler functionality.</td>
      <td>[Core Testing Docs](?path=/docs/testing-framework-tests-core-tests--docs)</td>
    </tr>
    <tr>
      <td><strong>Component Tests</strong></td>
      <td><code>test/wdio/</code></td>
      <td>WebdriverIO (<code>@wdio/browser-runner/stencil</code>)</td>
      <td>Verify individual features in isolation and satisfy static analysis.</td>
      <td>[Component Testing Docs](?path=/docs/testing-framework-tests-component-tests--docs)</td>
    </tr>
    <tr>
      <td><strong>E2E Tests</strong></td>
      <td><code>test/end-to-end/</code></td>
      <td>Stencil's <code>newE2EPage()</code> (Puppeteer)</td>
      <td>Verify that features work correctly together (e.g. multiple decorators, render function) and that component behavior is correct in a browser environment.</td>
      <td>[E2E Testing Docs](?path=/docs/testing-framework-tests-e2e-tests--docs)</td>
    </tr>
  </tbody>
</table>

## The 3-Step Testing Loop

The core testing workflow follows a systematic **3-step loop** that ensures comprehensive coverage as the Stencil framework evolves. This loop maintains synchronization between decorator functionality and test coverage:

### Step 1: Rules Discovery üîç
- **AI generates comprehensive matrix** of ALL possible permutations for a decorator
- **Generate components** from broad initial matrix (includes invalid combinations)
- **Compile components** to discover constraint violations through compiler errors
- **Create/update `rules.json`** to encode discovered constraints (e.g., mutually exclusive properties)
- **Refine matrix** based on rules, eliminating invalid permutations
- **Regenerate components** from validated matrix

### Step 2: Generate & Build üèóÔ∏è
- **Generate final components** from refined rules and validated matrix
- **Build components** successfully with no compiler errors
- **Validates** systematic coverage of all valid permutations
- **Creates** production-ready test components

### Step 3: Test & Verify ‚úÖ
- **Run tests** to verify runtime functionality (`npm run tests`)
- **Confirms** compiled components work correctly in browser environment
- **Validates** all decorator behaviors and permutations
- **Documents** test coverage and identifies gaps

### Loop Benefits
- **Automatic adaptation** to framework changes
- **Comprehensive coverage** maintained as decorators evolve  
- **Early detection** of breaking changes or missing coverage
- **Systematic verification** from generation ‚Üí compilation ‚Üí runtime

This 3-step loop replaces traditional ad-hoc testing approaches with a systematic, **discovery-driven** process that scales with framework complexity. The Rules Discovery phase is particularly powerful because it uses the compiler itself as the source of truth for valid decorator combinations.

## Quickstart
For a concise, step‚Äëby‚Äëstep path, see [Quickstart: Coverage ‚Üí Generate ‚Üí Verify](?path=/docs/testing-framework-quickstart--docs).

## Running Scripts with package.json Shortcuts

To streamline the testing workflow and avoid path resolution issues, a `package.json` file with shortcut scripts is provided in the `.ai/testing` folder. These scripts automatically `cd` into the correct directory and run the required commands, ensuring consistent execution regardless of your current working directory.

See also: Example Prompts at `.ai/testing/AGENT_PROMPTS.md`.

**How to use:**
- Refresh coverage for all decorators: `npm run refresh-coverage-matrix`
- Generate per-decorator components and re-run coverage: `npm run <decorator>:generate-components`
- Verify a decorator matrix and build overlays: `npm run <decorator>:verify-matrix`
- Build WDIO components: `npm run build-wdio-components`
- Run WDIO component tests: `npm run run-component-tests`
- Target a specific spec/glob: `npm run run-component-tests:spec -- ./state/**/*.test.tsx`
- Run a specific coverage script (e.g., component): `npm run component:coverage`

**Best Practice:**
Always use these shortcuts from the `.ai/testing` folder to avoid path issues and ensure scripts run in the correct environment. Update or add new shortcuts as needed to keep the workflow efficient and reliable.

## Recording Learnings

After each testing loop iteration, all learnings‚Äîincluding mistakes, improvements, coverage script gaps, and best practices‚Äîshould be documented in the appropriate `.ai` folder file for the feature or decorator. Additionally, a summary of key learnings and process changes should be added to the audit log (`audit.mdx`) to ensure institutional knowledge is preserved and future contributors benefit from past experience.

## Community-Driven Testing Ecosystem

Our testing framework creates a **crowdsourceable ecosystem** where the open source community can systematically contribute to test coverage. This approach scales testing quality through collaborative effort.

### Crowdsourcing Opportunities

**Gap Identification Dashboard**
- **Missing test cases** automatically identified in coverage reports
- **Visual matrix** showing which decorator/option combinations need tests  
- **Priority ranking** based on real-world usage patterns
- **Test coverage JSON** provides structured data for community analysis

**Community Contribution Pathways**
- **Easy entry point**: "Add a test case for X scenario"
- **Guided templates**: Follow existing patterns in structured test cases
- **AI-assisted implementation**: Contributors can use AI following established conventions
- **Model-agnostic**: Works with any AI model (proprietary or open-source)

**Expandable Test Scenarios**
Beyond current test cases, community can add:
- **Edge case testing** (null/undefined handling, boundary conditions)
- **Performance scenarios** (large data sets, memory usage)
- **Integration patterns** (component composition, complex interactions)
- **Browser compatibility** (specific browser behaviors, polyfills)
- **Accessibility scenarios** (screen reader support, keyboard navigation)
- **Real-world use cases** discovered in production applications

### Contribution Process
1. **Fork the repository** and identify gaps using coverage reports
2. **Follow the 3-step loop** with your preferred AI model or manual implementation
3. **Submit pull requests** with new test cases following established patterns
4. **Community review** ensures quality and consistency
5. **Automated verification** through systematic coverage tracking

This crowdsourcing approach harnesses collective intelligence to build more comprehensive test coverage than any single team could achieve alone.

### Test Case Documentation System

Our framework automatically generates comprehensive test case documentation:

**Structured Test Cases**: Each decorator follows consistent test case patterns
- **@Prop Test Cases**: 5 scenarios covering primitive runtime, complex render, reflect behaviors, mutable behaviors
- **@State Test Cases**: 2 scenarios covering primitive mutation and complex static render
- **Component naming**: Follows `{decorator}-{options}` convention for easy identification

**Coverage Tracking**: JSON reports provide detailed gap analysis
```json
{
  "coverage": { "covered": 24, "total": 24, "percent": "100.00" },
  "items": [
    {
      "group": "string",
      "options": { "type": "string", "reflect": "‚úì", "mutable": "‚úì" },
      "tested": true,
      "caseIds": [1, 3, 5]
    }
  ]
}
```

**Gap Identification**: Automated discovery of missing test coverage
- `"tested": false` entries indicate untested permutations
- Coverage percentages < 100% show improvement opportunities  
- Missing `caseIds` reveal scenarios needing implementation

**Community Accessibility**: Documentation designed for easy contribution
- Clear naming conventions enable pattern recognition
- Structured data supports automated gap analysis
- AI-friendly patterns facilitate assisted implementation
- Open source collaboration through systematic approach

## Continuous Improvement

This testing strategy is a living document. We will continue to refine our approach, adopt new tools, and improve our processes over time to ensure Stencil remains a reliable and high-quality framework for all developers.

A core tenet of this continuous improvement is the practice of documenting our learnings. After each major testing iteration, we will update the relevant documentation‚Äîincluding this strategy, the component-level and end-to-end testing guides, and the [audit log](?path=/docs/testing-framework-audit--docs)‚Äîwith any new insights, patterns, or challenges we encountered. This ensures that our institutional knowledge grows with our test suite.

**Key areas for learning documentation include:**

-   **AI Mistakes and Corrections**: Recording instances where the AI generated incorrect or suboptimal tests, along with the corrections made
-   **Coverage Script Gaps**: Documenting loopholes or blind spots discovered in coverage scripts and how they were addressed
-   **Documentation Improvements**: Noting areas where guides or instructions were unclear or incomplete
-   **Process Optimizations**: Capturing workflow improvements or shortcuts discovered during iterations
-   **Pattern Recognition**: Identifying recurring themes or best practices that emerge from the testing process

As this is our first time implementing the GenAI-powered loop, we anticipate a learning period where mistakes are possible. Our commitment is to learn from these experiences and rapidly evolve towards a rock-solid automated testing framework.

By adhering to this strategy, we build a comprehensive test suite that is both robust and maintainable, ensuring that Stencil remains a reliable tool for developers.

---

<div style={{ marginTop: '3rem', textAlign: 'center' }}>
  <span style={{
    fontSize: 36,
    fontWeight: 800,
    color: '#3b82f6',
    background: '#fff',
    borderRadius: 20,
    padding: '3rem 4rem',
    display: 'inline-block',
    boxShadow: '0 8px 32px rgba(59,130,246,0.10)',
    letterSpacing: '0.03em',
    border: '3px solid #0ea5e9',
    marginBottom: '2.5rem',
    textShadow: 'none',
    position: 'relative',
    overflow: 'hidden'
  }}>
    <span style={{ display: 'block', fontSize: 28, color: '#0ea5e9', fontWeight: 900, marginBottom: 18, zIndex: 1, position: 'relative', letterSpacing: '0.04em' }}>
      üöÄ [2025 Testing Roadmap: Dive into the future of Stencil.js testing!](?path=/docs/testing-framework-roadmap-2025--docs) üöÄ
    </span>
    <span style={{ display: 'block', fontSize: 20, color: '#6366f1', fontWeight: 500, marginTop: 18, zIndex: 1, position: 'relative', letterSpacing: '0.02em' }}>
      Next-gen coverage, GenAI-powered automation, and SSR mastery await.
    </span>
  </span>
</div>
